---
title: Continuous Batch in Orca
mathjax: true
toc: true
categories:
  - Study
tags:
  - LLM
---

Everyone talks about continuous batch with Anyscale blog, but the original [orca paper](https://www.usenix.org/system/files/osdi22-yu.pdf) and [talk](https://www.youtube.com/watch?v=Ob9PPLxETYU) gives some extra details

## 1 Request-level scheduling
Latency increase when there is finished request can not return due to other long output requests.
![Alt text](/assets/images/2025/25-06-05-orca_files/requestlevel.png)

## 2 Iteration-level scheduling
For `max_batch_size = 3` case
1. Process requests in execution engine (x12)
![Alt text](/assets/images/2025/25-06-05-orca_files/step1.png)
2. Move all requests in request pool together with new requsets (x123)
![Alt text](/assets/images/2025/25-06-05-orca_files/step2.png)
3. Process with new added requests x3
![Alt text](/assets/images/2025/25-06-05-orca_files/step3.png)
4. Finished requests (x2) can send back to reponses and process new added requests (x4)
![Alt text](/assets/images/2025/25-06-05-orca_files/step4.png)

## 3 Batch issues with iteration-level
For batching to work, we need following criteris
![Alt text](/assets/images/2025/25-06-05-orca_files/batch.png)
1. Batches are NOT in the same phase, prefill vs decode
![Alt text](/assets/images/2025/25-06-05-orca_files/diffphase.png)
2. Batches have different length
![Alt text](/assets/images/2025/25-06-05-orca_files/difflen.png)

## 4 Selective Batch
1. For LayerNorm and Linear layers, we can concatenate requests to a uniformed processing
![Alt text](/assets/images/2025/25-06-05-orca_files/cancat.png)
2. Split batch and process each request individually and merge the output tensor
![Alt text](/assets/images/2025/25-06-05-orca_files/atten.png)

## 5 Orca
Orca is the system with these two features implemented. For conitnuous  batching:
![Alt text](/assets/images/2025/25-06-05-orca_files/orca1.png)
For selective batching:
![Alt text](/assets/images/2025/25-06-05-orca_files/orca2.png)

