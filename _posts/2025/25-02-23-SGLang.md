---
title: SGLang
mathjax: true
toc: true
categories:
  - Study
tags:
  - LLM
---

How does [Structured Generation Language for LLM](https://arxiv.org/pdf/2312.07104) achieve such great performances and how is it differentiate from vLLM. 

Lianmin presented SGLang [here](https://www.youtube.com/watch?v=Ny4xxErgFgQ) explains key 4 technicals

## 0 History
SGLang history and milestones.
![Alt text](/assets/images/2025/25-02-23-SGLang_files/history.png)
![Alt text](/assets/images/2025/25-02-23-SGLang_files/milestone.png)
Key architecture is here, and will focus on server side
![Alt text](/assets/images/2025/25-02-23-SGLang_files/arch.png)

Highly recommend this [blog](https://zhuanlan.zhihu.com/p/711378550) talks about the positioning of SGLang. If vLLM is Caffe, SGLang is maybe Tensorflow? What will be the Pytorch of LLM infer framework ?

## 1 LM Programes
SGLang was designed to solve LM program constrains:
1. Multiple LLM calls 
2. Need constrained decoding.

![Alt text](/assets/images/2025/25-02-23-SGLang_files/sglang.png)

It has its own language primitives. It's similar to LMQL and Guidance
![Alt text](/assets/images/2025/25-02-23-SGLang_files/lmql.png)

Here is a full example using SGLang and it includes 3 key improvments (The 3rd one is for API calls only)
![Alt text](/assets/images/2025/25-02-23-SGLang_files/example.png)

## 2 RadixAttention 
KV cache is highly repeatable
![Alt text](/assets/images/2025/25-02-23-SGLang_files/kvcache.png)
The Radix Tree is a compressed prefix tree, and it's used to store KV cache for reuse.
![Alt text](/assets/images/2025/25-02-23-SGLang_files/radix.png)

## 3 Compressed FSM
FSM from **Outlines** is one common used method to force JSON schema. The limitation is only decode one token at a time. 
![Alt text](https://lmsys.org/images/blog/compressed_fsm/method1.png)
An improvement is from **Guidance** to employ interleaved-based decoding. A JSON schema can be breakdown into either a **chunked prefill** part or a **constrained decoding** part. The former can generate multiple tokens at one tim.
More details in this [blog](https://lmsys.org/blog/2024-02-05-compressed-fsm/)
![Alt text](/assets/images/2025/25-02-23-SGLang_files/interleaved.png)

Jump-Foward Decoding is a compressed FSM combining these two methods by compressing **singular transition edges**.
![Alt text](/assets/images/2025/25-02-23-SGLang_files/jfd.png)

## 4 Speculative API calls
This is a trival trick of **ignoring ending tokens** and it make API output more contents for possible future use by prompting. 

**Pytorch compile** is also one thing mentioned by Lianmin. and the following 3 optimization are not included in the orignal paper, and details are found in this [video](https://www.youtube.com/watch?v=XQylGyG7yp8) by Yineng Zhang. 

## 5 CPU Overlap Optimization
![Alt text](/assets/images/2025/25-02-23-SGLang_files/cpu.png)

## 6 FlashInfer Hopper Optimization and Integration
FlashInfer gives better perf than Triton implementation. 
![Alt text](/assets/images/2025/25-02-23-SGLang_files/flashinfer.png)
Key improvements from it are
![Alt text](/assets/images/2025/25-02-23-SGLang_files/fi1.png)
StreamK are SM level optimizations.
![Alt text](/assets/images/2025/25-02-23-SGLang_files/fi2.png)

## 7 TurboMind GEMM Optimization and Integration
![Alt text](/assets/images/2025/25-02-23-SGLang_files/turbomind.png)
LDSM is the CUDA code for "Load Matrix from Shared Memory with Element Size Expansion"
![Alt text](/assets/images/2025/25-02-23-SGLang_files/tm1.png)
Improvement on GEMM(GEneral Matrix to Matrix) is totally out of my knowledge


























