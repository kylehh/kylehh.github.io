---
title: SGLang
mathjax: true
toc: true
categories:
  - Study
tags:
  - LLM
---

How does [Structured Generation Language for LLM](https://arxiv.org/pdf/2312.07104)achieve such great performances and how does it differentiate from vLLM. 

Lianmin presented SGLang [here](https://www.youtube.com/watch?v=Ny4xxErgFgQ) explains key 4 technicals

## 0 History
SGLang history and milestones.
![Alt text](/assets/images/2025/25-02-23-SGLang_files/history.png)
![Alt text](/assets/images/2025/25-02-23-SGLang_files/milestone.png)
Key architecture is here, and will focus on server side
![Alt text](/assets/images/2025/25-02-23-SGLang_files/arch.png)

## 1 LM Programes
SGLang was designed to solve LM program constrains:
1. Multiple LLM calls 
2. Need constrained decoding.

![Alt text](/assets/images/2025/25-02-23-SGLang_files/sglang.png)

It has its own language primitives. It's similar to LMQL and Guidance
![Alt text](/assets/images/2025/25-02-23-SGLang_files/lmql.png)

Here is a full example using SGLang and it includes 3 key improvments (The 3rd one is for API calls only)
![Alt text](/assets/images/2025/25-02-23-SGLang_files/example.png)

## 2 RadixAttention 
KV cache is highly repeatable
![Alt text](/assets/images/2025/25-02-23-SGLang_files/kvcache.png)
The Radix Tree is a comporess prefix tree, and it's used to store KV cache for resue.
![Alt text](/assets/images/2025/25-02-23-SGLang_files/radix.png)

## 3 Compressed FSM
FSM from **Outlines** decode one token at a time. A improvement is from **Guidance** to do interleaved-based decoding.
![Alt text](/assets/images/2025/25-02-23-SGLang_files/interleaved.png)

Jump-Foward Decoding is a compressed FSM combineing these two methods
![Alt text](/assets/images/2025/25-02-23-SGLang_files/jfd.png)

## 4 Speculative API calls
This is a trival trick of ignore end token and let API output more contents for possible future use by prompting. 



The following 2 optimization are not included in the orignal paper, and details are found in this [video](https://www.youtube.com/watch?v=XQylGyG7yp8) by Yineng Zhang. 

## 5 CPU Overlap Optimization
![Alt text](/assets/images/2025/25-02-23-SGLang_files/cpu.png)

## 6 FlashInfer Hopper Optimization and Integration
FlashInfer gives better perf than Triton implementation. 
![Alt text](/assets/images/2025/25-02-23-SGLang_files/flashinfer.png)
Key improved from it are
![Alt text](/assets/images/2025/25-02-23-SGLang_files/fi1.png)
StreamK are SM level optimizations.
![Alt text](/assets/images/2025/25-02-23-SGLang_files/fi2.png)

## 7 TurboMind GEMM Optimization and Integration
![Alt text](/assets/images/2025/25-02-23-SGLang_files/turbomind.png)
LDSM is the CUDA code for "Load Matrix from Shared Memory with Element Size Expansion"
![Alt text](/assets/images/2025/25-02-23-SGLang_files/tm1.png)
Improvement on GEMM(GEneral Matrix to Matrix) is totally out of my knowledge


























