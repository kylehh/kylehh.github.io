---
title: ML101 - Self Attention
mathjax: true
toc: true
categories:
  - Study
tags:
  - ML
---


## 1
Feature normalization is important when different feature have different range. In general, it makes gradient descent converge faster.
![Alt text](/assets/images/23-08-20-ML101-SelfAttention_files/converge.png)


