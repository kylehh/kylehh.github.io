---
title: RL 2024-3
mathjax: true
toc: true
categories:
  - Study 
tags:
  - RL
---

Focus on PPO in this [post](https://cameronrwolfe.substack.com/p/proximal-policy-optimization-ppo) 

## 0 KL Divergence
KL divergence is actually related to entropy.
![Alt text](/assets/images/2024/24-03-29-RL24-3_files/entropy.png)
It's the loss functions for VAEs.
![Alt text](/assets/images/2024/24-03-29-RL24-3_files/kl.png)

## 1 Trust Region Policy Optimization (TRPO)
- Use a ratio, surrogate objective, between old and updated policies
- added constraint based on the KL divergence
- Solving a contrained maximization problem instead Gradient Ascent
![Alt text](/assets/images/2024/24-03-29-RL24-3_files/trpo.png)

Comparing to VPG, instead of working on parameter $\theta$ space, directly work on policies. Because small changes to $\theta$ can drastically alter the policy, ensuring that policy updates are small in the parameter space does not provide much of a guarantee on changes to the resulting policy. 

## 2 Proximal Policy Optimization (PPO)
Define surrogate objective for TRPO as below
![Alt text](/assets/images/2024/24-03-29-RL24-3_files/surrogate.png)
PPO's surragte objective is a clipped version
![Alt text](/assets/images/2024/24-03-29-RL24-3_files/ppo.png)