---
title: Llama2 tricks
mathjax: true
toc: true
categories:
  - Study
tags:
  - LLM
---

A good youtube [video](https://www.youtube.com/watch?v=Mn_9W1nCFLo) explained several tricks applied in Llama2  
![Alt text](/assets/images/2024/24-01-26-Llama2_files/summary.png)
Here is the study notes.
## 1. Layer normalization  
Batch norm: normalized by columns (same feature, different data)    
Layer norm: normalized by rows (same data, different features)  
It can help to solve **Internal Covariate Shift**, which means drastically changes in the output, and it leads to drastically changes in input in the next layer  
![Alt text](/assets/images/2024/24-01-26-Llama2_files/layernorm.png)
Rescale is more important than re-center, which leads to RMSnorm  
![Alt text](/assets/images/2024/24-01-26-Llama2_files/rmsnorm.png)
## 2. RoPE
RoPE(Rotary Position Encoding) is based on relative position encoding
![Alt text](/assets/images/2024/24-01-26-Llama2_files/absoluterelativeencoding.png)
Absolute encoding is added to embeddings  
![Alt text](/assets/images/2024/24-01-26-Llama2_files/addpositionalencoding.png)
Relative encoding is added to the key matrix, making it menage a trios
![Alt text](/assets/images/2024/24-01-26-Llama2_files/menageatrois.png)
RoPE is introduced in following paper, from a Chinese company called Zhuiyi（追一）
![Alt text](/assets/images/2024/24-01-26-Llama2_files/zhuiyi.png)
And when trying find a formula for the relative position, we actually introduced the rotary formula, that's how it gets the name
![Alt text](/assets/images/2024/24-01-26-Llama2_files/rotary.png)
## 3. KV Cache
To avoid repeatly calculation for previous tokens, we cache the calculated results
![Alt text](/assets/images/2024/24-01-26-Llama2_files/kvcache.png)
For regular inferences, tokens are repeatly calculated
![Alt text](/assets/images/2024/24-01-26-Llama2_files/reginf.png)
With KV cache, only the output token are put into next rounds' calculation
![Alt text](/assets/images/2024/24-01-26-Llama2_files/kvinf.png)
![Alt text](/assets/images/2024/24-01-26-Llama2_files/onetoken.png)
## 4.MQA
MQA(Multi-Query Attention) is paper from Noam Shazeer, who is the CEO of Character.ai, and author for transformers' paper and who invented multi-head attention.
![Alt text](/assets/images/2024/24-01-26-Llama2_files/MQA.png)
The GPU is too fast for the memory bandwidth to catchup
![Alt text](/assets/images/2024/24-01-26-Llama2_files/toofast.png)
This is NOT a problem for vanilla transformers
![Alt text](/assets/images/2024/24-01-26-Llama2_files/vanilla.png)
But for mutli-head transformers, which is used in practice, the ratio O(n/d+1/b) may become bottle neck
![Alt text](/assets/images/2024/24-01-26-Llama2_files/multihead.png)
MQA is the solution here, which removes h dimension from K and V, but only keeps in Q
![Alt text](/assets/images/2024/24-01-26-Llama2_files/multiquery.png) 
From the comparison below, you can see Grouped Multi-Query is the combination of multi-qeury and multi-head transformers.  
![Alt text](/assets/images/2024/24-01-26-Llama2_files/compare.png)

## 5. SwiGLU
This is from Noam again
![Alt text](/assets/images/2024/24-01-26-Llama2_files/swiglu.png)
It's actually an activation function with modification from Sigmoid.
![Alt text](/assets/images/2024/24-01-26-Llama2_files/activation.png)
Here is the most interesting results, why does this method work? It's **divine benevolence**
![Alt text](/assets/images/2024/24-01-26-Llama2_files/benevolence.png)


