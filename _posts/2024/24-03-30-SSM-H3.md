---
title: State Space Machine
mathjax: true
toc: true
categories:
  - Study
tags:
  - LLM
---

[Structured State Space for Sequence Modeliing S4](https://arxiv.org/abs/2111.00396) paper by Albert Gu, 2021

This is study note from this [blog](https://newsletter.maartengrootendorst.com/p/a-visual-guide-to-mamba-and-state)

## 0 The problem
We propose SSM to solve the slow inference issue for Transformers
![Alt text](/assets/images/2024/24-03-30-SSM-H3_files/issues.png)

## 1 State Space
State Space is vector representation of a state.
![Alt text](/assets/images/2024/24-03-30-SSM-H3_files/statevector.png)
In NN, *state* is typically hidden state, in LLM, it's generating a new token. 

## 2 State Space Model (SSM)
These two equations are the core of the State Space Model.
![Alt text](/assets/images/2024/24-03-30-SSM-H3_files/ssmc.png) 
and can be simplied as below with skip connection for matrix D
![Alt text](/assets/images/2024/24-03-30-SSM-H3_files/ssm2.png)
With discretization
![Alt text](/assets/images/2024/24-03-30-SSM-H3_files/ssmd.png)

## 3 RNN and CNN representation
It's very similar to RNN
![Alt text](/assets/images/2024/24-03-30-SSM-H3_files/ssmrnn.png)
and can use represented by CNN as well.
So 1D CNN for LLM with 1D kernel. 
![Alt text](/assets/images/2024/24-03-30-SSM-H3_files/cnn.png)

Now we have three representations
![Alt text](/assets/images/2024/24-03-30-SSM-H3_files/3.png) 
Combine the fast training for CNN and fast inference for RNN, we have **Linear State Space Layer**. 
![Alt text](/assets/images/2024/24-03-30-SSM-H3_files/lssl.png)

## 4 HiPPO and S4
In order to rememeber long context, HiPPO was introduced here. The core idea is focus on near term memory but also have inifinte long term memory w large degration. 
![Alt text](/assets/images/2024/24-03-30-SSM-H3_files/hippo.png)

Apply HiPPO to SSM, we have S4
![Alt text](/assets/images/2024/24-03-30-SSM-H3_files/s4.png)

Will discuss more details in the next blog.

## 5 H3
H3 (Hunry, Hunry HiPPO) is SSM sandwiched by two gated connections. It also inserts a standard local convolution, which they frame as a shift-SSM, before the main SSM layer.
![Alt text](/assets/images/2024/24-03-30-SSM-H3_files/h3.png) 

H3 is later envolved into Mamba. and 
there are some other architectures like **[Hyena](https://arxiv.org/pdf/2302.10866)**,**[RWKV](https://arxiv.org/pdf/2305.13048)**(Receptance Weighted Key Value) are all based on SSM.