---
title: SAM and BLIP
mathjax: true
toc: true
categories:
  - Study
tags:
  - CV
---

Segement Anything Model and Boostrap Lang-Image Pretraining

## 1 SAM 
Meta published SAM for a very general segmentation task.
![Alt text](/assets/images/2024/24-06-16-SAM_files/sampaper.png)

Couple of key technicals in SAM. SAM can take in various of input, points, box or text. 
![Alt text](/assets/images/2024/24-06-16-SAM_files/samoverview.png)
and use a **SA-1B** dataset which is high quality in segmentation and really large quantity.
![Alt text](/assets/images/2024/24-06-16-SAM_files/samdata.png)
1B data is impossible to have human label them all. So a bootstrapping loop was used here
![Alt text](/assets/images/2024/24-06-16-SAM_files/samstep.png)
Putting everything together, we get the fundation model
![Alt text](/assets/images/2024/24-06-16-SAM_files/sam.png)
Efficient SAM was introduced later 
![Alt text](/assets/images/2024/24-06-16-SAM_files/effisampaper.png)
Sacrifice accuracy for speed
![Alt text](/assets/images/2024/24-06-16-SAM_files/effisamres.png)
## 2 BLIP
What is Bootstrapping in AL
![Alt text](/assets/images/2024/24-06-16-SAM_files/bootstrapping.png) 

Let's see how BLIP is using this technology.
![Alt text](/assets/images/2024/24-06-16-SAM_files/blippaper.png)
First take a look at ALBEF(
ALign the image and text representations BEfore Fusing)implementation.Doing contrastive learning as the alignment between text and image embeddings
![Alt text](/assets/images/2024/24-06-16-SAM_files/albef.png)
BLIP has ITC is very similar to ALBEF, but also adding ITM(matching) and LM to output the text for image
![Alt text](/assets/images/2024/24-06-16-SAM_files/blip.png)
The training starts with internet data and small sets of human label as bootstrapping.
![Alt text](/assets/images/2024/24-06-16-SAM_files/bliptrain.png)
It can be used to correct title for a picture scrapping from internet $T_w$ with a synthetic $T_s$. The benefit is obvious, the previous title maybe irrelavant to the image but about personal feelings, and the corrected one is more useful for ML training.
![Alt text](/assets/images/2024/24-06-16-SAM_files/twts.png)

## 3 BLIP-2
Querying Transformer, as Q-Former is the contribution of BLIP-2.
![Alt text](/assets/images/2024/24-06-16-SAM_files/blip2paper.png)
![Alt text](/assets/images/2024/24-06-16-SAM_files/blip2overview.png)
First stage for fixed image encoder
![Alt text](/assets/images/2024/24-06-16-SAM_files/blip2detail.png)
Second stage for fixed LLM
![Alt text](/assets/images/2024/24-06-16-SAM_files/blip2.png)
Here are the results of BLIP-2
![Alt text](/assets/images/2024/24-06-16-SAM_files/blip2res.png)
