---
title: GenAI by Hung-yi Lee 2024-02
mathjax: true
toc: true
categories:
  - Study
tags:
  - LLM
---

Continue with Part 1, The 5th way of prompt engineering.
## 5 Model Cooperation
Model Cooperatation could be due to cost.
This is similar to MoE but no LLM architecture is changed here. 
![Alt text](/assets/images/2024/24-03-24-Lee2024-02_files/frugalGPT.png)
Or quality improvment based on reflect
![Alt text](/assets/images/2024/24-03-24-Lee2024-02_files/selfreflect.png)
Exchange-of-Throught is about different cooperatoin between model.s
![Alt text](/assets/images/2024/24-03-24-Lee2024-02_files/exchange.png)
The discussion is the longer the better. But on general, the models are very *polite* and no intend to argue. You need to purposely prompt them to get into discussion.
![Alt text](/assets/images/2024/24-03-24-Lee2024-02_files/discussstop.png)

Different roles can be assigned to models. But lots of spoiler for Frieren!
![Alt text](/assets/images/2024/24-03-24-Lee2024-02_files/roles.png)
and Dynamic LLM agent can also judge the performance of each LLM and exchange the low performing ones. 
![Alt text](/assets/images/2024/24-03-24-Lee2024-02_files/dynamic.png)

Couple of multi role Agents  
[MetaGPT](https://github.com/geekan/MetaGPT)    
[ChatDev](https://github.com/OpenBMB/ChatDev)  
[Generative Agent](https://github.com/joonspk-research/generative_agents)

Now Let's get into LLM training
## 1. Pretraining
Funny metaphor for different phase of training. 
![Alt text](/assets/images/2024/24-03-24-Lee2024-02_files/LLMtraining.png)
This part of talk is very basic, going through topics like hyperparameter and initialization. An interesting point is show how much data is needed to train **Syntactic**, **Semantic** and **Winograd** knowledge. 

Winograd Schema Challenge[WCS](https://en.wikipedia.org/wiki/Winograd_schema_challenge) is test of machine intelligence proposed in 2012 by Hector Levesque. 

The first cited example of a Winograd schema is due to Terry Winograd.
```
The city councilmen refused the demonstrators a permit because they [feared/advocated] violence.
```
The choices of "feared" and "advocated" turn the schema into its two instances:
```
The city councilmen refused the demonstrators a permit because they(concilmen) feared violence.
```
```
The city councilmen refused the demonstrators a permit because they(demonstrators) advocated violence.
```

Data engineering for LLM pre-training is mainly data cleaning.
![Alt text](/assets/images/2024/24-03-24-Lee2024-02_files/dataclean.png)
Otherwise some repeatation is shocking. 
![Alt text](/assets/images/2024/24-03-24-Lee2024-02_files/repeat.png)
Best takeaway from this talk, why pre-training is not good enough? Because the knowledge online may not direct answers. This is why SFT is used in the alignment.
![Alt text](/assets/images/2024/24-03-24-Lee2024-02_files/pretraining.png)