---
title: ML101 -3
mathjax: true
toc: true
categories:
  - study
tags:
  - ML
---
Start with Regression vs Classification. and introduce softmax
It seems there is long stories behind softmax, rather than normalization
![Alt text](/assets/images/23-08-20-ML101-3_files/softmax.png)
(Answer: Use Sigmoid for binary classification, which is equivalenet to Softmax in this case)
## 1 Loss functions
Cross-entropy is actually based on Maxi likelyhood method
![Alt text](/assets/images/23-08-20-ML101-3_files/crossenropy.png)

Cross-entropy surface is more smooth and not easy to be trapped by local min. 
![Alt text](/assets/images/23-08-20-ML101-3_files/crossenropy-2.png)

## 2 Batch Normalization